"""Evaluation framework for cleaning segment review flags.

This script evaluates the precision and recall of review flags generated by the cleaning LLM.
It processes sample documents and compares flagged segments against ground truth labels.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from src.app.container import get_app_container
from src.app.domain.models import Document


def _get_page_cleaning_metadata(cleaning_metadata: dict, page_number: int) -> dict:
    """Return metadata for a page regardless of whether keys are str or int."""
    if not cleaning_metadata:
        return {}
    return cleaning_metadata.get(page_number) or cleaning_metadata.get(str(page_number)) or {}


def load_ground_truth_labels(file_path: Path) -> dict[str, list[str]]:
    """Load ground truth labels from a JSON file.
    
    Expected format:
    {
        "document_id": ["segment_id1", "segment_id2", ...],
        ...
    }
    """
    if not file_path.exists():
        return {}
    with file_path.open() as f:
        return json.load(f)


def extract_flagged_segments(document: Document) -> list[dict[str, Any]]:
    """Extract all segments flagged for review from a document."""
    flagged_segments = []
    cleaning_metadata = document.metadata.get("cleaning_metadata_by_page", {}) if document.metadata else {}
    
    for page in document.pages:
        page_meta = _get_page_cleaning_metadata(cleaning_metadata, page.page_number)
        llm_segments = page_meta.get("llm_segments", {})
        segments = llm_segments.get("segments", [])
        
        for segment in segments:
            if segment.get("needs_review"):
                flagged_segments.append({
                    "document_id": document.id,
                    "page_number": page.page_number,
                    "segment_id": segment.get("segment_id"),
                    "text": segment.get("text", ""),
                    "rationale": segment.get("rationale"),
                })
    
    return flagged_segments


def calculate_metrics(
    flagged_segments: list[dict[str, Any]],
    ground_truth: list[str],
) -> dict[str, float]:
    """Calculate precision, recall, and F1 score.
    
    Args:
        flagged_segments: List of segments flagged by the LLM
        ground_truth: List of segment IDs that should have been flagged
    
    Returns:
        Dictionary with precision, recall, and f1_score
    """
    flagged_ids = {seg["segment_id"] for seg in flagged_segments if seg.get("segment_id")}
    ground_truth_set = set(ground_truth)
    
    # True positives: segments flagged that should be flagged
    tp = len(flagged_ids & ground_truth_set)
    
    # False positives: segments flagged that shouldn't be flagged
    fp = len(flagged_ids - ground_truth_set)
    
    # False negatives: segments not flagged that should be flagged
    fn = len(ground_truth_set - flagged_ids)
    
    # Calculate metrics
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "true_positives": tp,
        "false_positives": fp,
        "false_negatives": fn,
        "total_flagged": len(flagged_ids),
        "total_should_flag": len(ground_truth_set),
    }


def evaluate_document(
    document_id: str,
    ground_truth: list[str],
    container: Any,
) -> dict[str, Any]:
    """Evaluate review flags for a single document."""
    document = container.document_repository.get(document_id)
    if not document:
        return {"error": f"Document {document_id} not found"}
    
    flagged_segments = extract_flagged_segments(document)
    metrics = calculate_metrics(flagged_segments, ground_truth)
    
    return {
        "document_id": document_id,
        "filename": document.filename,
        "metrics": metrics,
        "flagged_segments": flagged_segments,
    }


def run_evaluation(
    ground_truth_file: Path | None = None,
    document_ids: list[str] | None = None,
) -> dict[str, Any]:
    """Run evaluation on sample documents.
    
    Args:
        ground_truth_file: Path to JSON file with ground truth labels
        document_ids: Optional list of document IDs to evaluate (if None, evaluates all)
    
    Returns:
        Evaluation results with metrics per document and aggregate metrics
    """
    container = get_app_container()
    
    # Load ground truth labels
    if ground_truth_file:
        ground_truth_all = load_ground_truth_labels(ground_truth_file)
    else:
        ground_truth_all = {}
    
    # Get documents to evaluate
    if document_ids:
        documents_to_eval = document_ids
    else:
        all_documents = container.document_repository.list()
        documents_to_eval = [doc.id for doc in all_documents]
    
    results = []
    for doc_id in documents_to_eval:
        ground_truth = ground_truth_all.get(doc_id, [])
        result = evaluate_document(doc_id, ground_truth, container)
        if "error" not in result:
            results.append(result)
    
    # Calculate aggregate metrics
    if results:
        total_tp = sum(r["metrics"]["true_positives"] for r in results)
        total_fp = sum(r["metrics"]["false_positives"] for r in results)
        total_fn = sum(r["metrics"]["false_negatives"] for r in results)
        
        aggregate_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        aggregate_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        aggregate_f1 = (
            2 * (aggregate_precision * aggregate_recall) / (aggregate_precision + aggregate_recall)
            if (aggregate_precision + aggregate_recall) > 0 else 0.0
        )
        
        aggregate_metrics = {
            "precision": aggregate_precision,
            "recall": aggregate_recall,
            "f1_score": aggregate_f1,
            "true_positives": total_tp,
            "false_positives": total_fp,
            "false_negatives": total_fn,
        }
    else:
        aggregate_metrics = {}
    
    return {
        "results": results,
        "aggregate_metrics": aggregate_metrics,
        "target_precision": 0.90,
        "target_recall": 0.80,
    }


if __name__ == "__main__":
    import sys
    
    # Example usage
    ground_truth_path = Path("tests/evaluation/ground_truth_labels.json")
    if len(sys.argv) > 1:
        ground_truth_path = Path(sys.argv[1])
    
    evaluation_results = run_evaluation(ground_truth_file=ground_truth_path)
    
    print("\n" + "=" * 80)
    print("Cleaning Review Flags Evaluation Results")
    print("=" * 80)
    
    if evaluation_results["aggregate_metrics"]:
        metrics = evaluation_results["aggregate_metrics"]
        print(f"\nAggregate Metrics:")
        print(f"  Precision: {metrics['precision']:.3f} (target: >0.90)")
        print(f"  Recall:    {metrics['recall']:.3f} (target: >0.80)")
        print(f"  F1 Score:  {metrics['f1_score']:.3f}")
        print(f"\n  True Positives:  {metrics['true_positives']}")
        print(f"  False Positives: {metrics['false_positives']}")
        print(f"  False Negatives: {metrics['false_negatives']}")
        
        # Check if targets are met
        precision_ok = metrics['precision'] >= evaluation_results['target_precision']
        recall_ok = metrics['recall'] >= evaluation_results['target_recall']
        
        print(f"\n  Targets Met:")
        print(f"    Precision >0.90: {'✓' if precision_ok else '✗'}")
        print(f"    Recall >0.80:    {'✓' if recall_ok else '✗'}")
    
    print(f"\nPer-Document Results:")
    for result in evaluation_results["results"]:
        m = result["metrics"]
        print(f"\n  {result['filename']} ({result['document_id'][:8]}...)")
        print(f"    Precision: {m['precision']:.3f}, Recall: {m['recall']:.3f}, F1: {m['f1_score']:.3f}")
        print(f"    Flagged: {m['total_flagged']}, Should flag: {m['total_should_flag']}")
    
    print("\n" + "=" * 80)
    
    # Save results to file
    output_path = Path("tests/evaluation/evaluation_results.json")
    with output_path.open("w") as f:
        json.dump(evaluation_results, f, indent=2)
    print(f"\nResults saved to: {output_path}")

