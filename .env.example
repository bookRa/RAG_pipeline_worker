# =============================================================================
# LOG LEVEL - Single source of truth for application logging
# =============================================================================
# Valid values: DEBUG, INFO, WARNING, ERROR, CRITICAL
#
# INFO (default): Clean progress logs - recommended for production
# DEBUG: Verbose diagnostic logs - recommended for development/debugging
#        Shows BCAI request details, base64 image info, token estimates, etc.
#
LOG_LEVEL=INFO

# =============================================================================
# LLM Configuration
# =============================================================================
# Options: openai, bcai, mock
LLM__PROVIDER=openai
LLM__MODEL=gpt-4o-mini
LLM__TEMPERATURE=0.1
LLM__API_KEY=your-openai-key
OPENAI_API_KEY=your-openai-key

# -----------------------------------------------------------------------------
# STREAMING vs STRUCTURED OUTPUTS - Understanding the Tradeoffs
# -----------------------------------------------------------------------------
#
# Both settings can be enabled independently. Here's what each combination does:
#
# | USE_STREAMING | USE_STRUCTURED_OUTPUTS | Behavior                           |
# |---------------|------------------------|-------------------------------------|
# | false         | true (recommended)     | Native JSON mode, MOST RELIABLE     |
# | true          | true                   | Progress logs, manual JSON parsing  |
# | false         | false                  | Fallback mode, not recommended      |
#
# STREAMING=false (Recommended for Production/BCAI):
#   - Uses native JSON mode (response_format) via LlamaIndex as_structured_llm()
#   - Provider enforces valid JSON at API level - MOST RELIABLE
#   - Full response logged after completion (at DEBUG level)
#   - No progress logs during parsing (waits for complete response)
#
# STREAMING=true (Recommended for Development):
#   - Progress logs every 5 seconds during parsing
#   - Guardrails detect infinite loops (repetition, excessive newlines)
#   - JSON schema injected into prompt, parsed manually - LESS RELIABLE
#   - Good for debugging and understanding what the LLM is generating
#
LLM__USE_STRUCTURED_OUTPUTS=true
LLM__USE_STREAMING=false

# Streaming guardrails (only apply when USE_STREAMING=true)
# LLM__STREAMING_MAX_CHARS=50000
# LLM__STREAMING_REPETITION_WINDOW=200
# LLM__STREAMING_REPETITION_THRESHOLD=0.8
# LLM__STREAMING_MAX_CONSECUTIVE_NEWLINES=100

# For BCAI:
# LLM__PROVIDER=bcai
# LLM__MODEL=gpt-4o-mini
# LLM__API_BASE=https://bcai-test.web.boeing.com
# LLM__API_KEY=your-bcai-pat
# LLM__CONVERSATION_MODE=non-rag
# LLM__CONVERSATION_SOURCE=rag-pipeline-worker
# LLM__USE_STREAMING=false  # Recommended for BCAI - more reliable
# BCAI_API_KEY=your-bcai-pat
# BCAI_API_BASE=https://bcai-test.web.boeing.com

# Embeddings
# Options: openai, bcai, mock
EMBEDDINGS__PROVIDER=openai
EMBEDDINGS__MODEL=text-embedding-3-small

# For BCAI embeddings (uses same credentials as LLM by default):
# EMBEDDINGS__PROVIDER=bcai
# EMBEDDINGS__MODEL=text-embedding-3-small
# EMBEDDINGS__DIMENSIONS=1536

# Chunking defaults
CHUNKING__SPLITTER=sentence
CHUNKING__CHUNK_SIZE=512
CHUNKING__CHUNK_OVERLAP=50
CHUNKING__INCLUDE_IMAGES=true

# Vector store
VECTOR_STORE__DRIVER=llama_index_local
VECTOR_STORE__PERSIST_DIR=artifacts/vector_store

# Prompt files (override if you relocate them)
PROMPTS__PARSING_SYSTEM_PROMPT_PATH=docs/prompts/parsing/system.md
PROMPTS__PARSING_USER_PROMPT_PATH=docs/prompts/parsing/user.md
PROMPTS__CLEANING_SYSTEM_PROMPT_PATH=docs/prompts/cleaning/system.md
PROMPTS__CLEANING_USER_PROMPT_PATH=docs/prompts/cleaning/user.md
PROMPTS__SUMMARY_PROMPT_PATH=docs/prompts/summarization/system.md

# Storage overrides (optional)
INGESTION_STORAGE_DIR=artifacts/ingestion
DOCUMENT_STORAGE_DIR=artifacts/documents
RUN_ARTIFACTS_DIR=artifacts/runs

# Langfuse observability (optional)
ENABLE_LANGFUSE=false
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
LANGFUSE_HOST=https://us.cloud.langfuse.com

# DocumentDB configuration
VECTOR_STORE__DRIVER=documentdb
DOCUMENTDB_URI=mongodb://...
DOCUMENTDB_DATABASE=pipeline_db

# Batch processing configuration
BATCH__MAX_CONCURRENT_DOCUMENTS=5
BATCH__MAX_WORKERS_PER_DOCUMENT=4
BATCH__ENABLE_PAGE_PARALLELISM=true
BATCH__ENABLE_DOCUMENT_PARALLELISM=true
BATCH__RATE_LIMIT_REQUESTS_PER_MINUTE=60
BATCH__PIXMAP_PARALLEL_WORKERS=4